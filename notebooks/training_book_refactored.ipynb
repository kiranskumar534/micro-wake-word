{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz92hE-cJUk0"
      },
      "source": [
        "# Training a microWakeWord Model for ESP32-S3-BOX3\n",
        "\n",
        "This notebook guides you through training a wake word detection model for Malayalam (‡¥∞‡¥æ‡¥ò‡¥µ‡¥æ).\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- **Python**: 3.10 (required)\n",
        "- **GPU**: T4 or better (recommended for faster training)\n",
        "- **Disk Space**: ~15GB free\n",
        "- **RAM**: 16GB+ recommended\n",
        "- **Time**: 2-4 hours total\n",
        "\n",
        "## üîÑ Workflow Overview\n",
        "\n",
        "1. **Setup** (5 min): Install dependencies and validate environment\n",
        "2. **TTS Setup** (10 min): Download Piper TTS and voice models\n",
        "3. **Sample Generation** (20-30 min): Generate 1000 wake word samples\n",
        "4. **Augmentation Data** (30-60 min): Download background audio datasets\n",
        "5. **Feature Generation** (20-30 min): Create training spectrograms\n",
        "6. **Training** (1-2 hours): Train the model\n",
        "7. **Export** (5 min): Export TFLite model for ESP32\n",
        "\n",
        "## ‚ö†Ô∏è Important Notes\n",
        "\n",
        "- Run cells **in order** from top to bottom\n",
        "- Don't skip cells unless explicitly marked optional\n",
        "- Large downloads will show progress bars\n",
        "- Training can be resumed if interrupted\n",
        "\n",
        "## üéØ Expected Output\n",
        "\n",
        "A quantized TFLite model file (~200-500KB) ready for deployment to ESP32-S3-BOX3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhE7rbOqJUk2"
      },
      "source": [
        "---\n",
        "## Step 1: Environment Setup & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcpHcC9OJUk3"
      },
      "outputs": [],
      "source": [
        "# Install microWakeWord and dependencies\n",
        "import sys\n",
        "import platform\n",
        "print(\"üîß Installing dependencies...\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "# Check Python version\n",
        "if sys.version_info[:2] != (3, 10):\n",
        "    print(\"‚ö†Ô∏è  WARNING: Python 3.10 is recommended. Current version may cause issues.\")\n",
        "# Install audio-metadata from fork (fixes attrs dependency)\n",
        "!pip install -q 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
        "# Clone and install microWakeWord\n",
        "import os\n",
        "if not os.path.exists('./microWakeWord'):\n",
        "    print(\"üì• Cloning microWakeWord...\")\n",
        "    !git clone https://github.com/kahrendt/microWakeWord\n",
        "else:\n",
        "    print(\"‚úÖ microWakeWord directory already exists\")\n",
        "# Always install/reinstall to ensure it's available\n",
        "print(\"üì¶ Installing microWakeWord package...\")\n",
        "!pip install -e ./microWakeWord\n",
        "# Verify installation\n",
        "print(\"\\nüîç Verifying installation...\")\n",
        "try:\n",
        "    import microwakeword\n",
        "    print(\"‚úÖ microwakeword module successfully imported!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå ERROR: Could not import microwakeword: {e}\")\n",
        "print(\"\\n‚úÖ Dependencies installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "199FAITTJUk4"
      },
      "outputs": [],
      "source": [
        "# Validate environment and check resources\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "print(\"üîç Environment Validation\\n\" + \"=\"*50)\n",
        "\n",
        "# Check disk space\n",
        "total, used, free = shutil.disk_usage(\".\")\n",
        "free_gb = free // (2**30)\n",
        "print(f\"üíæ Free disk space: {free_gb} GB\")\n",
        "if free_gb < 15:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Less than 15GB free. You may run out of space.\")\n",
        "else:\n",
        "    print(\"‚úÖ Sufficient disk space\")\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"\\nüéÆ GPU: {gpu_name}\")\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "    print(\"‚úÖ GPU acceleration available\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No GPU detected. Training will be VERY slow.\")\n",
        "    print(\"   Consider using Google Colab with GPU runtime.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Environment validation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOWRB2SNJUk4"
      },
      "source": [
        "---\n",
        "## Step 2: Configuration\n",
        "\n",
        "**All configuration in one place** - modify these values as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m-XHu7TJUk5"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Modify these values as needed\n",
        "# ============================================================================\n",
        "\n",
        "# Wake word configuration\n",
        "TARGET_WORD = \"‡¥∞‡¥æ‡¥ò‡¥µ‡¥æ\"  # Malayalam wake word\n",
        "\n",
        "# TTS Models (Malayalam voices)\n",
        "TTS_MODELS = {\n",
        "    \"meera\": {\n",
        "        \"name\": \"ml_IN-meera-medium\",\n",
        "        \"path\": \"ml/ml_IN/meera/medium\"\n",
        "    },\n",
        "    \"arjun\": {\n",
        "        \"name\": \"ml_IN-arjun-medium\",\n",
        "        \"path\": \"ml/ml_IN/arjun/medium\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sample generation settings\n",
        "SAMPLES_PER_MODEL = 25  # Total: 1000 samples (500 per voice)\n",
        "MAX_WORKERS = 8  # Parallel workers for sample generation (adjust for your CPU)\n",
        "\n",
        "# TTS variation parameters\n",
        "LENGTH_SCALES = [1.2, 1.3, 1.4, 1.5, 1.6]  # Speech speed variations\n",
        "NOISE_SCALES = [0.5, 0.667, 0.8, 1.0]  # Voice variation\n",
        "\n",
        "# Augmentation settings\n",
        "AUGMENTATION_DURATION_S = 3.2\n",
        "BACKGROUND_MIN_SNR_DB = -5\n",
        "BACKGROUND_MAX_SNR_DB = 10\n",
        "\n",
        "# Training settings\n",
        "TRAINING_STEPS = 10000  # Increase for better quality (but longer training)\n",
        "BATCH_SIZE = 128  # Adjust based on GPU memory (T4 can handle 128)\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Directory structure\n",
        "DIRS = {\n",
        "    \"piper\": \"./piper_standalone\",\n",
        "    \"models\": \"./models\",\n",
        "    \"samples\": \"./generated_samples\",\n",
        "    \"mit_rirs\": \"./mit_rirs\",\n",
        "    \"esc50\": \"./esc50_16k\",\n",
        "    \"fma\": \"./fma_16k\",\n",
        "    \"features\": \"./generated_augmented_features\",\n",
        "    \"negative\": \"./negative_datasets\",\n",
        "    \"trained\": \"./trained_models/wakeword\"\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"\\nüìù Target word: {TARGET_WORD}\")\n",
        "print(f\"üé§ TTS models: {', '.join(TTS_MODELS.keys())}\")\n",
        "print(f\"üìä Total samples to generate: {SAMPLES_PER_MODEL * len(TTS_MODELS)}\")\n",
        "print(f\"üîß Parallel workers: {MAX_WORKERS}\")\n",
        "print(f\"üéØ Training steps: {TRAINING_STEPS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hix_V1kGJUk5"
      },
      "source": [
        "---\n",
        "## Step 3: Directory Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQSM0NUWJUk5"
      },
      "outputs": [],
      "source": [
        "# Create all required directories\n",
        "import os\n",
        "\n",
        "print(\"üìÅ Creating directory structure...\\n\")\n",
        "\n",
        "for name, path in DIRS.items():\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"‚úÖ {name:12} ‚Üí {path}\")\n",
        "\n",
        "# Create subdirectories for features\n",
        "for split in [\"training\", \"validation\", \"testing\"]:\n",
        "    os.makedirs(os.path.join(DIRS[\"features\"], split), exist_ok=True)\n",
        "\n",
        "print(\"\\n‚úÖ Directory structure created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWZmiGuJUk5"
      },
      "source": [
        "---\n",
        "## Step 4: Download Piper TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpABKpWXJUk5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Clean up existing broken installations\n",
        "if os.path.exists(\"piper_standalone\"):\n",
        "    print(\"üóëÔ∏è Removing existing piper_standalone to ensure a clean install...\")\n",
        "    shutil.rmtree(\"piper_standalone\")\n",
        "\n",
        "# 2. Download and Extract\n",
        "print(\"üì• Downloading Piper TTS...\")\n",
        "!wget -q https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_amd64.tar.gz\n",
        "!tar -xf piper_amd64.tar.gz\n",
        "\n",
        "# 3. Rename the extracted folder 'piper' to 'piper_standalone'\n",
        "if os.path.exists(\"piper\"):\n",
        "    os.rename(\"piper\", \"piper_standalone\")\n",
        "    print(\"‚úÖ Piper folder renamed to piper_standalone\")\n",
        "else:\n",
        "    print(\"‚ùå Error: Extracted folder 'piper' not found!\")\n",
        "\n",
        "# 4. Clean up the archive\n",
        "if os.path.exists(\"piper_amd64.tar.gz\"):\n",
        "    os.remove(\"piper_amd64.tar.gz\")\n",
        "\n",
        "# 5. Verify and set permissions\n",
        "piper_exe = \"./piper_standalone/piper\"\n",
        "if os.path.exists(piper_exe):\n",
        "    os.chmod(piper_exe, 0o755)\n",
        "    print(f\"‚ú® SUCCESS: Piper is ready at {piper_exe}\")\n",
        "    !{piper_exe} --version\n",
        "else:\n",
        "    print(\"üîç Binary not found in expected spot. Searching...\")\n",
        "    !find . -name \"piper\" -type f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpo9Z86kJUk6"
      },
      "source": [
        "---\n",
        "## Step 5: Download TTS Voice Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qekUyZDaJUk6"
      },
      "outputs": [],
      "source": [
        "# Download Malayalam voice models\n",
        "import os\n",
        "\n",
        "print(\"üì• Downloading TTS voice models...\\n\")\n",
        "\n",
        "for voice_id, config in TTS_MODELS.items():\n",
        "    model_name = config[\"name\"]\n",
        "    model_path = config[\"path\"]\n",
        "\n",
        "    onnx_file = f\"{DIRS['models']}/{model_name}.onnx\"\n",
        "    json_file = f\"{DIRS['models']}/{model_name}.onnx.json\"\n",
        "\n",
        "    if os.path.exists(onnx_file) and os.path.exists(json_file):\n",
        "        print(f\"‚úÖ {voice_id:6} model already exists\")\n",
        "    else:\n",
        "        print(f\"üì• Downloading {voice_id} model...\")\n",
        "        !wget -q -L -O {onnx_file} \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path}/{model_name}.onnx\"\n",
        "        !wget -q -L -O {json_file} \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path}/{model_name}.onnx.json\"\n",
        "        print(f\"‚úÖ {voice_id} model downloaded\")\n",
        "\n",
        "print(\"\\n‚úÖ All TTS models ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaHbNqW_JUk6"
      },
      "source": [
        "---\n",
        "## Step 6: Generate Wake Word Samples\n",
        "\n",
        "This will generate 1000 samples (500 per voice) with variations in speed and tone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCAJMzAeJUk6"
      },
      "outputs": [],
      "source": [
        "# Generate wake word samples in parallel\n",
        "import os\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Piper executable path\n",
        "piper_exe = \"./piper_standalone/piper.exe\" if os.name == 'nt' else \"./piper_standalone/piper\"\n",
        "\n",
        "# Progress tracking\n",
        "progress_lock = Lock()\n",
        "progress_counters = {}\n",
        "failed_samples = []\n",
        "\n",
        "def generate_sample(model_name, model_path, sample_idx, output_dir):\n",
        "    \"\"\"Generate a single wake word sample\"\"\"\n",
        "    length = LENGTH_SCALES[sample_idx % len(LENGTH_SCALES)]\n",
        "    noise = NOISE_SCALES[sample_idx % len(NOISE_SCALES)]\n",
        "    output_file = f\"{output_dir}/{sample_idx}.wav\"\n",
        "\n",
        "    try:\n",
        "        cmd = f'echo {TARGET_WORD} | \"{piper_exe}\" --model \"{model_path}\" --length_scale {length} --noise_scale {noise} --output_file \"{output_file}\"'\n",
        "\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            return True, None\n",
        "        else:\n",
        "            return False, f\"Sample {sample_idx}: {result.stderr[:100]}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, f\"Sample {sample_idx}: {str(e)[:100]}\"\n",
        "\n",
        "# Generate samples for each model\n",
        "print(f\"üéôÔ∏è  Generating {SAMPLES_PER_MODEL * len(TTS_MODELS)} wake word samples...\\n\")\n",
        "\n",
        "for voice_id, config in TTS_MODELS.items():\n",
        "    model_name = config[\"name\"]\n",
        "    model_path = f\"{DIRS['models']}/{model_name}.onnx\"\n",
        "    output_dir = f\"{DIRS['samples']}/{voice_id}\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"üé§ Generating {SAMPLES_PER_MODEL} samples for {voice_id}...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = []\n",
        "\n",
        "        for i in range(SAMPLES_PER_MODEL):\n",
        "            future = executor.submit(\n",
        "                generate_sample,\n",
        "                voice_id,\n",
        "                model_path,\n",
        "                i,\n",
        "                output_dir\n",
        "            )\n",
        "            futures.append(future)\n",
        "\n",
        "        # Progress bar\n",
        "        success_count = 0\n",
        "        with tqdm(total=SAMPLES_PER_MODEL, desc=f\"{voice_id}\") as pbar:\n",
        "            for future in as_completed(futures):\n",
        "                success, error = future.result()\n",
        "                if success:\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    failed_samples.append((voice_id, error))\n",
        "                pbar.update(1)\n",
        "\n",
        "        print(f\"‚úÖ {voice_id}: {success_count}/{SAMPLES_PER_MODEL} samples generated\\n\")\n",
        "\n",
        "# Summary\n",
        "total_expected = SAMPLES_PER_MODEL * len(TTS_MODELS)\n",
        "total_failed = len(failed_samples)\n",
        "total_success = total_expected - total_failed\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Sample generation complete!\")\n",
        "print(f\"   Success: {total_success}/{total_expected}\")\n",
        "if total_failed > 0:\n",
        "    print(f\"   Failed: {total_failed}\")\n",
        "    print(f\"\\n‚ö†Ô∏è  First 5 errors:\")\n",
        "    for voice, error in failed_samples[:5]:\n",
        "        print(f\"   [{voice}] {error}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN0DCXQ9JUk6"
      },
      "outputs": [],
      "source": [
        "# Validate generated samples\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üîç Validating generated samples...\\n\")\n",
        "\n",
        "for voice_id in TTS_MODELS.keys():\n",
        "    sample_dir = Path(DIRS[\"samples\"]) / voice_id\n",
        "    wav_files = list(sample_dir.glob(\"*.wav\"))\n",
        "    print(f\"‚úÖ {voice_id:6}: {len(wav_files)} WAV files\")\n",
        "\n",
        "total_samples = len(list(Path(DIRS[\"samples\"]).rglob(\"*.wav\")))\n",
        "print(f\"\\nüìä Total samples: {total_samples}\")\n",
        "\n",
        "if total_samples < 100:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Very few samples generated. Training may not work well.\")\n",
        "    print(\"   Consider regenerating samples or checking for errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ7u3pDLJUk6"
      },
      "source": [
        "---\n",
        "## Step 7: Download Augmentation Data\n",
        "\n",
        "**This step downloads background audio for data augmentation:**\n",
        "- MIT RIR: Room impulse responses (~500 files, ~50MB)\n",
        "- ESC-50: Environmental sounds (50 files, ~100MB)\n",
        "- FMA: Music dataset (OPTIONAL, 1000 files, ~2GB)\n",
        "\n",
        "**Total download: ~150MB (or ~2GB with FMA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n81ildoGJUk7"
      },
      "outputs": [],
      "source": [
        "# Download MIT RIR dataset from direct source\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np  # ADDED: Missing import\n",
        "\n",
        "if not os.path.exists(DIRS[\"mit_rirs\"]) or len(list(Path(DIRS[\"mit_rirs\"]).glob(\"*.wav\"))) == 0:\n",
        "    print(\"üì• Downloading MIT RIR dataset from direct source...\")\n",
        "\n",
        "    try:\n",
        "        # Download from MIT's direct link\n",
        "        url = \"https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip\"\n",
        "        zip_path = \"mit_rir.zip\"\n",
        "\n",
        "        print(\"   Downloading...\")\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "        print(\"   Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"mit_rir_temp\")\n",
        "\n",
        "        # Convert to 16kHz WAV files\n",
        "        import librosa\n",
        "        import scipy.io.wavfile\n",
        "\n",
        "        os.makedirs(DIRS[\"mit_rirs\"], exist_ok=True)\n",
        "        audio_files = list(Path(\"mit_rir_temp\").rglob(\"*.wav\"))[:500]\n",
        "\n",
        "        count = 0\n",
        "        for audio_file in tqdm(audio_files, desc=\"Converting to 16kHz\"):\n",
        "            try:\n",
        "                audio, sr = librosa.load(str(audio_file), sr=16000, mono=True)\n",
        "                output_name = f\"rir_{count:04d}.wav\"\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(DIRS[\"mit_rirs\"], output_name),\n",
        "                    16000,\n",
        "                    (audio * 32767).astype(np.int16)\n",
        "                )\n",
        "                count += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Cleanup\n",
        "        import shutil\n",
        "        shutil.rmtree(\"mit_rir_temp\")\n",
        "        os.remove(zip_path)\n",
        "\n",
        "        print(f\"‚úÖ MIT RIR: {count} files downloaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  MIT RIR download failed: {e}\")\n",
        "        print(\"   Continuing without MIT RIR data...\")\n",
        "else:\n",
        "    rir_count = len(list(Path(DIRS[\"mit_rirs\"]).glob(\"*.wav\")))\n",
        "    print(f\"‚úÖ MIT RIR already exists ({rir_count} files)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYb5Qm66JUk7"
      },
      "outputs": [],
      "source": [
        "# Download ESC-50 environmental sounds\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np  # ADDED: Missing import\n",
        "\n",
        "if not os.path.exists(DIRS[\"esc50\"]) or len(list(Path(DIRS[\"esc50\"]).glob(\"*.wav\"))) == 0:\n",
        "    print(\"üì• Downloading ESC-50 environmental sounds...\")\n",
        "\n",
        "    try:\n",
        "        # Download\n",
        "        url = \"https://github.com/karolpiczak/ESC-50/archive/master.zip\"\n",
        "        zip_path = \"esc50.zip\"\n",
        "\n",
        "        print(\"   Downloading...\")\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "        # Extract\n",
        "        print(\"   Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "\n",
        "        os.rename(\"ESC-50-master\", \"esc50_temp\")\n",
        "\n",
        "        # Convert to 16kHz\n",
        "        import librosa\n",
        "        import scipy.io.wavfile\n",
        "\n",
        "        os.makedirs(DIRS[\"esc50\"], exist_ok=True)\n",
        "        wav_files = list(Path(\"esc50_temp/audio\").glob(\"*.wav\"))\n",
        "\n",
        "        count = 0\n",
        "        for wav_file in tqdm(wav_files, desc=\"Converting to 16kHz\"):\n",
        "            try:\n",
        "                audio, sr = librosa.load(str(wav_file), sr=16000, mono=True)\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(DIRS[\"esc50\"], wav_file.name),\n",
        "                    16000,\n",
        "                    (audio * 32767).astype(np.int16)\n",
        "                )\n",
        "                count += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Cleanup\n",
        "        import shutil\n",
        "        shutil.rmtree(\"esc50_temp\")\n",
        "        os.remove(zip_path)\n",
        "\n",
        "        print(f\"‚úÖ ESC-50: {count} files downloaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  ESC-50 download failed: {e}\")\n",
        "else:\n",
        "    esc_count = len(list(Path(DIRS[\"esc50\"]).glob(\"*.wav\")))\n",
        "    print(f\"‚úÖ ESC-50 already exists ({esc_count} files)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8ty1GfEJUk7"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Download FMA music dataset (WARNING: ~2GB download)\n",
        "# Uncomment the code below if you want to include music in augmentation\n",
        "\n",
        "DOWNLOAD_FMA = False  # Set to True to download FMA\n",
        "\n",
        "if DOWNLOAD_FMA:\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    if not os.path.exists(DIRS[\"fma\"]) or len(list(Path(DIRS[\"fma\"]).glob(\"*.wav\"))) == 0:\n",
        "        print(\"üì• Downloading FMA dataset (WARNING: ~2GB, this will take time)...\")\n",
        "\n",
        "        # Download and process FMA\n",
        "        # (Implementation similar to ESC-50 but with larger dataset)\n",
        "        print(\"   This is a large download. Consider skipping if not needed.\")\n",
        "    else:\n",
        "        fma_count = len(list(Path(DIRS[\"fma\"]).glob(\"*.wav\")))\n",
        "        print(f\"‚úÖ FMA already exists ({fma_count} files)\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping FMA download (set DOWNLOAD_FMA=True to include)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23SAvG9-JUk7"
      },
      "outputs": [],
      "source": [
        "# Summary of augmentation data\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üìä Augmentation Data Summary\\n\" + \"=\"*50)\n",
        "\n",
        "mit_count = len(list(Path(DIRS[\"mit_rirs\"]).glob('*.wav'))) if os.path.exists(DIRS[\"mit_rirs\"]) else 0\n",
        "esc50_count = len(list(Path(DIRS[\"esc50\"]).glob('*.wav'))) if os.path.exists(DIRS[\"esc50\"]) else 0\n",
        "fma_count = len(list(Path(DIRS[\"fma\"]).glob('*.wav'))) if os.path.exists(DIRS[\"fma\"]) else 0\n",
        "\n",
        "print(f\"  MIT RIRs:        {mit_count:4} files\")\n",
        "print(f\"  ESC-50:          {esc50_count:4} files\")\n",
        "print(f\"  FMA:             {fma_count:4} files\")\n",
        "print(f\"  {'‚îÄ'*46}\")\n",
        "print(f\"  Total:           {mit_count + esc50_count + fma_count:4} files\")\n",
        "\n",
        "if mit_count + esc50_count + fma_count == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No augmentation data downloaded!\")\n",
        "    print(\"   Training may not work well without background audio.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Augmentation data ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOJDRXSTJUk7"
      },
      "source": [
        "---\n",
        "## Step 8: Setup Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yPLX2g0JUk7"
      },
      "outputs": [],
      "source": [
        "# Setup audio clips and augmentation pipeline\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üîß Setting up augmentation pipeline...\\n\")\n",
        "\n",
        "# Install torchcodec if needed\n",
        "try:\n",
        "    import torchcodec\n",
        "    print(\"‚úÖ torchcodec already installed\\n\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing torchcodec for audio decoding...\")\n",
        "    !pip install -q torchcodec\n",
        "    print(\"‚úÖ torchcodec installed\")\n",
        "    print(\"\\n‚ö†Ô∏è  IMPORTANT: Please restart the kernel and re-run from this cell!\")\n",
        "    print(\"   (Runtime ‚Üí Restart runtime, then re-run this cell)\\n\")\n",
        "    raise SystemExit(\"Kernel restart required after torchcodec installation\")\n",
        "\n",
        "# Verify samples exist\n",
        "samples_dir = DIRS[\"samples\"]\n",
        "total_samples = len(list(Path(samples_dir).rglob(\"*.wav\")))\n",
        "print(f\"üìä Found {total_samples} WAV files in {samples_dir}\\n\")\n",
        "\n",
        "if total_samples == 0:\n",
        "    print(\"‚ùå ERROR: No samples found! Re-run Step 6\")\n",
        "else:\n",
        "    # Setup clips from generated samples\n",
        "    print(\"Loading clips...\")\n",
        "    clips = Clips(\n",
        "        input_directory=DIRS[\"samples\"],\n",
        "        file_pattern='**/*.wav',\n",
        "        max_clip_duration_s=None,\n",
        "        remove_silence=False,\n",
        "        random_split_seed=10,\n",
        "        split_count=0.1,\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Clips loaded: {len(clips.clips)} total\")\n",
        "\n",
        "    # Handle both dict and object-based clips\n",
        "    try:\n",
        "        train_count = len([c for c in clips.clips if c.get('split') == 'train' or c.get('split') == 0])\n",
        "        val_count = len([c for c in clips.clips if c.get('split') == 'validation' or c.get('split') == 1])\n",
        "        test_count = len([c for c in clips.clips if c.get('split') == 'test' or c.get('split') == 2])\n",
        "    except:\n",
        "        train_count = len([c for c in clips.clips if c.split == 'train'])\n",
        "        val_count = len([c for c in clips.clips if c.split == 'validation'])\n",
        "        test_count = len([c for c in clips.clips if c.split == 'test'])\n",
        "\n",
        "    print(f\"   Train: {train_count}\")\n",
        "    print(f\"   Validation: {val_count}\")\n",
        "    print(f\"   Test: {test_count}\\n\")\n",
        "\n",
        "    # Determine background paths\n",
        "    background_paths = []\n",
        "    if os.path.exists(DIRS[\"esc50\"]) and len(list(Path(DIRS[\"esc50\"]).glob(\"*.wav\"))) > 0:\n",
        "        background_paths.append(DIRS[\"esc50\"])\n",
        "    if os.path.exists(DIRS[\"fma\"]) and len(list(Path(DIRS[\"fma\"]).glob(\"*.wav\"))) > 0:\n",
        "        background_paths.append(DIRS[\"fma\"])\n",
        "\n",
        "    # Determine impulse paths\n",
        "    impulse_paths = []\n",
        "    if os.path.exists(DIRS[\"mit_rirs\"]) and len(list(Path(DIRS[\"mit_rirs\"]).glob(\"*.wav\"))) > 0:\n",
        "        impulse_paths.append(DIRS[\"mit_rirs\"])\n",
        "\n",
        "    print(f\"üìÅ Background audio: {', '.join(background_paths) if background_paths else 'None'}\")\n",
        "    print(f\"üìÅ Impulse responses: {', '.join(impulse_paths) if impulse_paths else 'None'}\")\n",
        "\n",
        "    if not background_paths and not impulse_paths:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: No augmentation data available!\")\n",
        "        print(\"   Training will proceed with limited augmentation.\")\n",
        "\n",
        "    # Setup augmentation\n",
        "    augmenter = Augmentation(\n",
        "        augmentation_duration_s=AUGMENTATION_DURATION_S,\n",
        "        augmentation_probabilities={\n",
        "            \"SevenBandParametricEQ\": 0.1,\n",
        "            \"TanhDistortion\": 0.1,\n",
        "            \"PitchShift\": 0.1,\n",
        "            \"BandStopFilter\": 0.1,\n",
        "            \"AddColorNoise\": 0.1,\n",
        "            \"AddBackgroundNoise\": 0.75 if background_paths else 0.0,\n",
        "            \"Gain\": 1.0,\n",
        "            \"RIR\": 0.5 if impulse_paths else 0.0,\n",
        "        },\n",
        "        impulse_paths=impulse_paths if impulse_paths else None,\n",
        "        background_paths=background_paths if background_paths else None,\n",
        "        background_min_snr_db=BACKGROUND_MIN_SNR_DB,\n",
        "        background_max_snr_db=BACKGROUND_MAX_SNR_DB,\n",
        "        min_jitter_s=0.195,\n",
        "        max_jitter_s=0.205,\n",
        "    )\n",
        "    print(\"‚úÖ Augmentation configured\")\n",
        "\n",
        "    print(\"\\n‚úÖ Augmentation pipeline ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BD2nqEHJUk8"
      },
      "source": [
        "---\n",
        "## Step 9: Generate Training Features\n",
        "\n",
        "This creates spectrogram features for training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SoEzLJ3JUk8"
      },
      "outputs": [],
      "source": [
        "# Generate augmented spectrogram features\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "import os\n",
        "from pathlib import Path\n",
        "print(\"üé® Generating training features...\\n\")\n",
        "# Verify clips exist\n",
        "print(f\"üìä Clips summary:\")\n",
        "print(f\"   Total clips: {len(clips.clips)}\")\n",
        "# Count by split\n",
        "try:\n",
        "    train_clips = [c for c in clips.clips if c.get('split') in ['train', 0]]\n",
        "    val_clips = [c for c in clips.clips if c.get('split') in ['validation', 1]]\n",
        "    test_clips = [c for c in clips.clips if c.get('split') in ['test', 2]]\n",
        "except:\n",
        "    train_clips = [c for c in clips.clips if c.split == 'train']\n",
        "    val_clips = [c for c in clips.clips if c.split == 'validation']\n",
        "    test_clips = [c for c in clips.clips if c.split == 'test']\n",
        "print(f\"   Train clips: {len(train_clips)}\")\n",
        "print(f\"   Validation clips: {len(val_clips)}\")\n",
        "print(f\"   Test clips: {len(test_clips)}\\n\")\n",
        "if len(clips.clips) == 0:\n",
        "    print(\"‚ùå ERROR: No clips found! Check Step 8 output\")\n",
        "else:\n",
        "    splits_config = [\n",
        "        {\"name\": \"training\", \"split\": \"train\", \"repetition\": 2, \"slide_frames\": 10},\n",
        "        {\"name\": \"validation\", \"split\": \"validation\", \"repetition\": 1, \"slide_frames\": 10},\n",
        "        {\"name\": \"testing\", \"split\": \"test\", \"repetition\": 1, \"slide_frames\": 1},\n",
        "    ]\n",
        "\n",
        "    for config in splits_config:\n",
        "        split_name = config[\"name\"]\n",
        "        out_dir = os.path.join(DIRS[\"features\"], split_name)\n",
        "\n",
        "        print(f\"üìä Generating {split_name} features...\")\n",
        "\n",
        "        try:\n",
        "            spectrograms = SpectrogramGeneration(\n",
        "                clips=clips,\n",
        "                augmenter=augmenter,\n",
        "                slide_frames=config[\"slide_frames\"],\n",
        "                step_ms=10,\n",
        "            )\n",
        "\n",
        "            RaggedMmap.from_generator(\n",
        "                out_dir=os.path.join(out_dir, 'wakeword_mmap'),\n",
        "                sample_generator=spectrograms.spectrogram_generator(\n",
        "                    split=config[\"split\"],\n",
        "                    repeat=config[\"repetition\"]\n",
        "                ),\n",
        "                batch_size=100,\n",
        "                verbose=True,\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ {split_name} features complete\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ERROR in {split_name}: {e}\\n\")\n",
        "\n",
        "    print(\"‚úÖ All training features generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqHfj9B4JUk8"
      },
      "source": [
        "## Verify Features\n",
        "This step verifies that the features are setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uKiuPhIJUk8"
      },
      "outputs": [],
      "source": [
        "# Verify generated features\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"üîç Verifying generated features...\\n\")\n",
        "\n",
        "for split in [\"training\", \"validation\", \"testing\"]:\n",
        "    mmap_dir = os.path.join(DIRS[\"features\"], split, \"wakeword_mmap\")\n",
        "\n",
        "    if os.path.exists(mmap_dir):\n",
        "        files = list(Path(mmap_dir).glob(\"*\"))\n",
        "        print(f\"‚úÖ {split:12}: {len(files)} files in {mmap_dir}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {split:12}: Directory not found: {mmap_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "total_features = len(list(Path(DIRS[\"features\"]).rglob(\"*\")))\n",
        "print(f\"Total feature files: {total_features}\")\n",
        "\n",
        "if total_features < 10:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Very few feature files generated!\")\n",
        "    print(\"   Training may fail. Check Step 9 output for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STP8teFJJUk8"
      },
      "source": [
        "---\n",
        "## Step 10: Download Negative Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibAcZxd1JUk8"
      },
      "outputs": [],
      "source": [
        "# Download pre-generated negative datasets\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DIRS[\"negative\"]) or len(os.listdir(DIRS[\"negative\"])) == 0:\n",
        "    print(\"üì• Downloading negative datasets...\\n\")\n",
        "\n",
        "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
        "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
        "\n",
        "    for fname in filenames:\n",
        "        link = link_root + fname\n",
        "        zip_path = f\"{DIRS['negative']}/{fname}\"\n",
        "\n",
        "        print(f\"üì• Downloading {fname}...\")\n",
        "        !wget -q -O {zip_path} {link}\n",
        "        !unzip -q {zip_path} -d {DIRS['negative']}\n",
        "        os.remove(zip_path)\n",
        "        print(f\"‚úÖ {fname} extracted\")\n",
        "\n",
        "    print(\"\\n‚úÖ Negative datasets downloaded!\")\n",
        "else:\n",
        "    print(\"‚úÖ Negative datasets already exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek4fg20bJUk9"
      },
      "source": [
        "---\n",
        "## Step 11: Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhO8v5wCJUk9"
      },
      "outputs": [],
      "source": [
        "# Generate training configuration YAML\n",
        "import yaml\n",
        "import os\n",
        "print(\"üìù Generating training configuration...\\n\")\n",
        "config = {\n",
        "    \"window_step_ms\": 10,\n",
        "    \"train_dir\": DIRS[\"trained\"],\n",
        "\n",
        "    \"features\": [\n",
        "        {\n",
        "            \"features_dir\": os.path.join(DIRS[\"features\"], \"training\", \"wakeword_mmap\"),\n",
        "            \"sampling_weight\": 2.0,\n",
        "            \"penalty_weight\": 1.0,\n",
        "            \"truth\": True,\n",
        "            \"truncation_strategy\": \"truncate_start\",\n",
        "            \"type\": \"mmap\",\n",
        "        },\n",
        "        {\n",
        "            \"features_dir\": f\"{DIRS['negative']}/speech\",\n",
        "            \"sampling_weight\": 10.0,\n",
        "            \"penalty_weight\": 1.0,\n",
        "            \"truth\": False,\n",
        "            \"truncation_strategy\": \"random\",\n",
        "            \"type\": \"mmap\",\n",
        "        },\n",
        "        {\n",
        "            \"features_dir\": f\"{DIRS['negative']}/dinner_party\",\n",
        "            \"sampling_weight\": 10.0,\n",
        "            \"penalty_weight\": 1.0,\n",
        "            \"truth\": False,\n",
        "            \"truncation_strategy\": \"random\",\n",
        "            \"type\": \"mmap\",\n",
        "        },\n",
        "        {\n",
        "            \"features_dir\": f\"{DIRS['negative']}/no_speech\",\n",
        "            \"sampling_weight\": 5.0,\n",
        "            \"penalty_weight\": 1.0,\n",
        "            \"truth\": False,\n",
        "            \"truncation_strategy\": \"random\",\n",
        "            \"type\": \"mmap\",\n",
        "        },\n",
        "        {\n",
        "            \"features_dir\": f\"{DIRS['negative']}/dinner_party_eval\",\n",
        "            \"sampling_weight\": 0.0,\n",
        "            \"penalty_weight\": 1.0,\n",
        "            \"truth\": False,\n",
        "            \"truncation_strategy\": \"split\",\n",
        "            \"type\": \"mmap\",\n",
        "        },\n",
        "    ],\n",
        "\n",
        "    \"training_steps\": [TRAINING_STEPS],\n",
        "    \"positive_class_weight\": [1],\n",
        "    \"negative_class_weight\": [20],\n",
        "    \"learning_rates\": [LEARNING_RATE],\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "\n",
        "    \"time_mask_max_size\": [0],\n",
        "    \"time_mask_count\": [0],\n",
        "    \"freq_mask_max_size\": [0],\n",
        "    \"freq_mask_count\": [0],\n",
        "\n",
        "    \"eval_step_interval\": 500,\n",
        "    \"clip_duration_ms\": 1500,\n",
        "\n",
        "    \"target_minimization\": 0.9,\n",
        "    \"minimization_metric\": None,\n",
        "    \"maximization_metric\": \"average_viable_recall\",\n",
        "}\n",
        "# Save configuration\n",
        "config_path = \"training_parameters.yaml\"\n",
        "with open(config_path, \"w\") as file:\n",
        "    yaml.dump(config, file)\n",
        "print(f\"‚úÖ Configuration saved to {config_path}\")\n",
        "print(f\"\\nüìä Settings:\")\n",
        "print(f\"   Steps: {TRAINING_STEPS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"\\nüìÅ Paths:\")\n",
        "print(f\"   Wake word: {os.path.join(DIRS['features'], 'training', 'wakeword_mmap')}\")\n",
        "print(f\"   Negative: {DIRS['negative']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Fcq_k_JUk9"
      },
      "source": [
        "---\n",
        "## Step 12: Train Model\n",
        "\n",
        "**This will take 1-2 hours on T4 GPU.**\n",
        "\n",
        "The training will:\n",
        "- Save checkpoints every 500 steps\n",
        "- Evaluate on validation set\n",
        "- Select best model based on recall\n",
        "- Quantize and convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-U9ZCulJUk9"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"üöÄ Starting model training...\\n\")\n",
        "print(\"‚è±Ô∏è  Expected time: 1-2 hours on T4 GPU\")\n",
        "print(\"üí° You can monitor progress in the output below\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!python -m microwakeword.model_train_eval \\\n",
        "--training_config='training_parameters.yaml' \\\n",
        "--train 1 \\\n",
        "--restore_checkpoint 1 \\\n",
        "--test_tf_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming_quantized 0 \\\n",
        "--test_tflite_streaming 0 \\\n",
        "--test_tflite_streaming_quantized 1 \\\n",
        "--use_weights \"best_weights\" \\\n",
        "mixednet \\\n",
        "--pointwise_filters \"64,64,64,64\" \\\n",
        "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "--residual_connection \"0,0,0,0\" \\\n",
        "--first_conv_filters 32 \\\n",
        "--first_conv_kernel_size 5 \\\n",
        "--stride 3\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvFhioKzJUk9"
      },
      "source": [
        "---\n",
        "## Step 13: Export Model\n",
        "\n",
        "The trained model is ready for deployment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inAbXZpqJUk9"
      },
      "outputs": [],
      "source": [
        "# Verify and export the model\n",
        "import os\n",
        "\n",
        "model_path = f\"{DIRS['trained']}/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    model_size = os.path.getsize(model_path) / 1024  # KB\n",
        "    print(\"‚úÖ Model exported successfully!\\n\")\n",
        "    print(f\"üìÅ Model location: {model_path}\")\n",
        "    print(f\"üìä Model size: {model_size:.1f} KB\")\n",
        "\n",
        "    # Download for Google Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"\\nüì• Downloading model file...\")\n",
        "        files.download(model_path)\n",
        "        print(\"‚úÖ Model downloaded!\")\n",
        "    except:\n",
        "        print(\"\\nüí° Running locally - model saved to disk\")\n",
        "        print(f\"   Copy from: {model_path}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: Model file not found!\")\n",
        "    print(f\"   Expected at: {model_path}\")\n",
        "    print(\"   Check training output for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF478xXLJUk9"
      },
      "source": [
        "---\n",
        "## üéâ Next Steps\n",
        "\n",
        "### 1. Create Model Manifest JSON\n",
        "\n",
        "Create a JSON file for ESPHome (e.g., `raghava.json`):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"type\": \"micro\",\n",
        "  \"wake_word\": \"raghava\",\n",
        "  \"author\": \"Your Name\",\n",
        "  \"website\": \"https://github.com/yourusername/your-repo\",\n",
        "  \"model\": \"stream_state_internal_quant.tflite\",\n",
        "  \"version\": 1,\n",
        "  \"micro\": {\n",
        "    \"probability_cutoff\": 0.5,\n",
        "    \"sliding_window_average_size\": 10\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### 2. Test the Model\n",
        "\n",
        "Before deploying to ESP32, test with audio files to verify it works.\n",
        "\n",
        "### 3. Deploy to ESP32-S3-BOX3\n",
        "\n",
        "1. Copy both files to your ESPHome config directory:\n",
        "   - `stream_state_internal_quant.tflite`\n",
        "   - `raghava.json`\n",
        "\n",
        "2. Update your ESPHome YAML:\n",
        "```yaml\n",
        "micro_wake_word:\n",
        "  models:\n",
        "    - model: raghava.json\n",
        "```\n",
        "\n",
        "3. Flash to ESP32-S3-BOX3\n",
        "\n",
        "### 4. Tune Performance\n",
        "\n",
        "Adjust `probability_cutoff` in the JSON:\n",
        "- **Too many false positives**: Increase cutoff (e.g., 0.6, 0.7)\n",
        "- **Doesn't detect wake word**: Decrease cutoff (e.g., 0.4, 0.3)\n",
        "\n",
        "### 5. Improve Model (if needed)\n",
        "\n",
        "If the model doesn't work well:\n",
        "- Generate more samples (increase `SAMPLES_PER_MODEL`)\n",
        "- Train longer (increase `TRAINING_STEPS`)\n",
        "- Adjust augmentation parameters\n",
        "- Record real voice samples and add to training data\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Resources\n",
        "\n",
        "- [ESPHome microWakeWord Documentation](https://esphome.io/components/micro_wake_word)\n",
        "- [Model Repository Examples](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2)\n",
        "- [microWakeWord GitHub](https://github.com/kahrendt/microWakeWord)\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! You've successfully trained a wake word model! üéâ**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}