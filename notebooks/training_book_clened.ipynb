{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11cNiLqvWC6"
      },
      "source": [
        "# Training a microWakeWord Model\n",
        "\n",
        "This notebook steps you through training a basic microWakeWord model. It is intended as a **starting point** for advanced users. You should use Python 3.10.\n",
        "\n",
        "**The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**\n",
        "\n",
        "In the comment at the start of certain blocks, I note some specific settings to consider modifying.\n",
        "\n",
        "This runs on Google Colab, but is extremely slow compared to training on a local GPU. If you must use Colab, be sure to Change the runtime type to a GPU. Even then, it still slow!\n",
        "\n",
        "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFf6511E65ff",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Installs microWakeWord. Be sure to restart the session after this is finished.\n",
        "import platform\n",
        "\n",
        "if platform.system() == \"Darwin\":\n",
        "    # `pymicro-features` is installed from a fork to support building on macOS\n",
        "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
        "\n",
        "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
        "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
        "\n",
        "!git clone https://github.com/kahrendt/microWakeWord\n",
        "!pip install -e ./microWakeWord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEluu7nL7ywd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# 1. Configuration\n",
        "target_word = \"‡¥∞‡¥æ‡¥ò‡¥µ‡¥æ\"\n",
        "model_name = \"ml_IN-meera-medium\"\n",
        "model_path_hf = \"ml/ml_IN/meera/medium\"\n",
        "\n",
        "# Download Logic (Standard)\n",
        "if not os.path.exists(\"./piper_standalone\"):\n",
        "    !wget -q https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_amd64.tar.gz\n",
        "    !tar -xf piper_amd64.tar.gz\n",
        "    !mv piper piper_standalone\n",
        "\n",
        "!mkdir -p models\n",
        "!wget -q -L -O models/{model_name}.onnx \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path_hf}/{model_name}.onnx\"\n",
        "!wget -q -L -O models/{model_name}.onnx.json \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path_hf}/{model_name}.onnx.json\"\n",
        "\n",
        "target_word = \"‡¥∞‡¥æ‡¥ò‡¥µ‡¥æ\"\n",
        "model_name = \"ml_IN-arjun-medium\"\n",
        "model_path_hf = \"ml/ml_IN/arjun/medium\"\n",
        "!wget -q -L -O models/{model_name}.onnx \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path_hf}/{model_name}.onnx\"\n",
        "!wget -q -L -O models/{model_name}.onnx.json \"https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_path_hf}/{model_name}.onnx.json\"\n",
        "\n",
        "\n",
        "\n",
        "# 2. Generate variations with SLOWER speeds\n",
        "# 1.0 is normal. We are testing 1.1 to 1.5 to slow it down.\n",
        "print(f\"Generating slower Malayalam variations for: {target_word}\")\n",
        "os.makedirs(\"malayalam_slow_test\", exist_ok=True)\n",
        "\n",
        "# Testing different 'slowness' levels\n",
        "slow_speeds = [1.0, 1.15, 1.25, 1.35, 1.45]\n",
        "\n",
        "for i, speed in enumerate(slow_speeds):\n",
        "    output_file = f\"malayalam_slow_test/slow_{speed}.wav\"\n",
        "    # Using --length_scale to slow down the voice\n",
        "    !echo '{target_word}' | ./piper_standalone/piper \\\n",
        "        --model models/{model_name}.onnx \\\n",
        "        --length_scale {speed} \\\n",
        "        --output_file {output_file}\n",
        "\n",
        "# 3. Playback\n",
        "print(\"\\n--- Malayalam Audio Results (Slowed Down) ---\")\n",
        "for speed in slow_speeds:\n",
        "    file_path = f\"malayalam_slow_test/slow_{speed}.wav\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Length Scale: {speed} ({'Normal' if speed==1.0 else 'Slower'})\")\n",
        "        display(Audio(file_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SvGtCCM9akR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "\n",
        "# Configuration\n",
        "target_word = \"‡¥∞‡¥æ‡¥ò‡¥µ‡¥æ\"\n",
        "models = {\n",
        "    \"meera\": \"models/ml_IN-meera-medium.onnx\",\n",
        "    \"arjun\": \"models/ml_IN-arjun-medium.onnx\"\n",
        "}\n",
        "samples_per_model = 500\n",
        "max_workers = 8  # Number of parallel processes - adjust based on your CPU\n",
        "\n",
        "# Variations\n",
        "length_scales = [1.2, 1.3, 1.4, 1.5, 1.6]\n",
        "noise_scales = [0.5, 0.667, 0.8, 1.0]\n",
        "\n",
        "# Piper executable\n",
        "piper_exe = \"./piper_standalone/piper.exe\" if os.name == 'nt' else \"./piper_standalone/piper\"\n",
        "\n",
        "# Progress tracking\n",
        "progress_lock = Lock()\n",
        "progress_counters = {}\n",
        "\n",
        "def generate_sample(model_name, model_path, sample_idx, output_dir):\n",
        "    \"\"\"Generate a single sample\"\"\"\n",
        "    length = length_scales[sample_idx % len(length_scales)]\n",
        "    noise = noise_scales[sample_idx % len(noise_scales)]\n",
        "    output_file = f\"{output_dir}/{sample_idx}.wav\"\n",
        "\n",
        "    try:\n",
        "        # Use subprocess for better control\n",
        "        cmd = f'echo {target_word} | \"{piper_exe}\" --model \"{model_path}\" --length_scale {length} --noise_scale {noise} --output_file \"{output_file}\"'\n",
        "\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            # Update progress\n",
        "            with progress_lock:\n",
        "                progress_counters[model_name] += 1\n",
        "                current = progress_counters[model_name]\n",
        "                if current % 50 == 0:\n",
        "                    print(f\"  [{model_name}] Progress: {current}/{samples_per_model}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è [{model_name}] Sample {sample_idx} failed: {result.stderr}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è [{model_name}] Exception at sample {sample_idx}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main generation loop\n",
        "print(f\"üöÄ Starting parallel generation with {max_workers} workers...\")\n",
        "\n",
        "for model_name, model_path in models.items():\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"‚ö†Ô∏è Skipping {model_name}: Model not found\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüéôÔ∏è Generating {samples_per_model} samples for {model_name}...\")\n",
        "\n",
        "    output_dir = f\"generated_samples/{model_name}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize progress counter\n",
        "    progress_counters[model_name] = 0\n",
        "\n",
        "    # Generate samples in parallel\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = []\n",
        "\n",
        "        for i in range(samples_per_model):\n",
        "            future = executor.submit(\n",
        "                generate_sample,\n",
        "                model_name,\n",
        "                model_path,\n",
        "                i,\n",
        "                output_dir\n",
        "            )\n",
        "            futures.append(future)\n",
        "\n",
        "        # Wait for all to complete\n",
        "        success_count = 0\n",
        "        for future in as_completed(futures):\n",
        "            if future.result():\n",
        "                success_count += 1\n",
        "\n",
        "    print(f\"‚úÖ [{model_name}] Completed: {success_count}/{samples_per_model} samples\")\n",
        "\n",
        "print(f\"\\n‚úÖ All samples generated in ./generated_samples/\")\n",
        "print(f\"üìä Total samples: {sum(progress_counters.values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJRG4Qvo9nXG"
      },
      "outputs": [],
      "source": [
        "# Downloads audio data for augmentation. This can be slow!\n",
        "# **Google Colab optimized with working download links**\n",
        "\n",
        "# Install required audio libraries\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q soundfile librosa audioread ffmpeg-python\n",
        "\n",
        "import datasets\n",
        "import scipy\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "import soundfile as sf\n",
        "\n",
        "# Progress callback for downloads\n",
        "def download_with_progress(url, output_path):\n",
        "    \"\"\"Download with progress bar\"\"\"\n",
        "    print(f\"üì• Starting download from {url.split('/')[-1]}...\")\n",
        "\n",
        "    class DownloadProgressBar(tqdm):\n",
        "        def update_to(self, b=1, bsize=1, tsize=None):\n",
        "            if tsize is not None:\n",
        "                self.total = tsize\n",
        "            self.update(b * bsize - self.n)\n",
        "\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, output_path, reporthook=t.update_to)\n",
        "    print(\"‚úÖ Download complete!\")\n",
        "\n",
        "## Download MIT RIR data\n",
        "output_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"üì• Loading MIT RIR dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Use non-streaming mode for reliability\n",
        "        rir_dataset = datasets.load_dataset(\n",
        "            \"davidscripka/MIT_environmental_impulse_responses\",\n",
        "            split=\"train[:500]\",  # Limit to 500 samples\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Save clips to 16-bit PCM wav files\n",
        "        count = 0\n",
        "        for row in tqdm(rir_dataset, desc=\"Processing RIR files\"):\n",
        "            try:\n",
        "                name = row['audio']['path'].split('/')[-1]\n",
        "                audio_array = row['audio']['array']\n",
        "                sample_rate = row['audio']['sampling_rate']\n",
        "\n",
        "                # Resample to 16kHz if needed\n",
        "                if sample_rate != 16000:\n",
        "                    import librosa\n",
        "                    audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(output_dir, name),\n",
        "                    16000,\n",
        "                    (audio_array * 32767).astype(np.int16)\n",
        "                )\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ MIT RIR dataset complete: {count} files\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è MIT RIR dataset download failed: {e}\")\n",
        "        print(\"Continuing without MIT RIR data...\")\n",
        "else:\n",
        "    print(\"‚úÖ MIT RIRs already exist, skipping\")\n",
        "\n",
        "## Download FMA using direct file processing instead of datasets library\n",
        "if not os.path.exists(\"fma_16k\"):\n",
        "    os.makedirs(\"fma\", exist_ok=True)\n",
        "    fname = \"fma_xs.zip\"\n",
        "\n",
        "    # Alternative: Use smaller FMA dataset from different source\n",
        "    link = \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\"\n",
        "    out_path = os.path.join(\"fma\", \"fma_small.zip\")\n",
        "\n",
        "    print(f\"üì• Downloading FMA dataset (~7.2GB - this will take time on Colab)...\")\n",
        "    print(\"‚ö†Ô∏è This is a large download. Consider using pre-generated features instead.\")\n",
        "\n",
        "    try:\n",
        "        download_with_progress(link, out_path)\n",
        "\n",
        "        # Extract\n",
        "        print(\"üì¶ Extracting FMA (this may take 5-10 minutes)...\")\n",
        "        with zipfile.ZipFile(out_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"fma\")\n",
        "\n",
        "        # Convert to 16kHz using librosa directly\n",
        "        output_dir = \"./fma_16k\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        mp3_files = list(Path(\"fma/fma_small\").glob(\"**/*.mp3\"))[:1000]  # Limit to 1000 files\n",
        "        print(f\"Converting {len(mp3_files)} FMA files to 16kHz using librosa...\")\n",
        "\n",
        "        import librosa\n",
        "        for idx, mp3_file in enumerate(tqdm(mp3_files, desc=\"Converting FMA to 16kHz\")):\n",
        "            try:\n",
        "                # Load with librosa (handles mp3 without torchcodec)\n",
        "                audio, sr = librosa.load(str(mp3_file), sr=16000, mono=True)\n",
        "\n",
        "                # Save as wav\n",
        "                name = mp3_file.stem + \".wav\"\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(output_dir, name),\n",
        "                    16000,\n",
        "                    (audio * 32767).astype(np.int16)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                if idx % 100 == 0:\n",
        "                    print(f\"‚ö†Ô∏è Error at file {idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ FMA conversion complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è FMA download/processing failed: {e}\")\n",
        "        print(\"Continuing without FMA data...\")\n",
        "else:\n",
        "    print(\"‚úÖ FMA already exists, skipping\")\n",
        "\n",
        "## Alternative: Download pre-generated background noise from other sources\n",
        "# Using ESC-50 environmental sounds as alternative to audioset\n",
        "output_dir = \"./esc50\"\n",
        "if not os.path.exists(output_dir):\n",
        "    print(\"üì• Downloading ESC-50 environmental sounds (alternative to AudioSet)...\")\n",
        "\n",
        "    try:\n",
        "        link = \"https://github.com/karolpiczak/ESC-50/archive/master.zip\"\n",
        "        out_path = \"esc50.zip\"\n",
        "\n",
        "        download_with_progress(link, out_path)\n",
        "\n",
        "        print(\"üì¶ Extracting ESC-50...\")\n",
        "        with zipfile.ZipFile(out_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "\n",
        "        os.rename(\"ESC-50-master\", \"esc50\")\n",
        "\n",
        "        # Convert to 16kHz\n",
        "        output_dir = \"./esc50_16k\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        wav_files = list(Path(\"esc50/audio\").glob(\"*.wav\"))\n",
        "        print(f\"Converting {len(wav_files)} ESC-50 files to 16kHz...\")\n",
        "\n",
        "        import librosa\n",
        "        for idx, wav_file in enumerate(tqdm(wav_files, desc=\"Converting ESC-50\")):\n",
        "            try:\n",
        "                audio, sr = librosa.load(str(wav_file), sr=16000, mono=True)\n",
        "                name = wav_file.name\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(output_dir, name),\n",
        "                    16000,\n",
        "                    (audio * 32767).astype(np.int16)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ ESC-50 complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ESC-50 download failed: {e}\")\n",
        "else:\n",
        "    print(\"‚úÖ ESC-50 already exists, skipping\")\n",
        "\n",
        "print(\"\\n‚úÖ Augmentation audio data download complete!\")\n",
        "print(\"üìä Summary:\")\n",
        "mit_count = len(list(Path('./mit_rirs').glob('*.wav'))) if os.path.exists('./mit_rirs') else 0\n",
        "esc50_count = len(list(Path('./esc50_16k').glob('*.wav'))) if os.path.exists('./esc50_16k') else 0\n",
        "fma_count = len(list(Path('./fma_16k').glob('*.wav'))) if os.path.exists('./fma_16k') else 0\n",
        "\n",
        "print(f\"  - MIT RIRs: {mit_count} files\")\n",
        "print(f\"  - ESC-50 (16kHz): {esc50_count} files\")\n",
        "print(f\"  - FMA (16kHz): {fma_count} files\")\n",
        "print(f\"  - Total: {mit_count + esc50_count + fma_count} files\")\n",
        "\n",
        "if mit_count + esc50_count + fma_count == 0:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: No background audio downloaded!\")\n",
        "    print(\"Consider using pre-downloaded datasets or smaller alternatives.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3bmbI5-JAz"
      },
      "outputs": [],
      "source": [
        "# Sets up the augmentations.\n",
        "# To improve your model, experiment with these settings and use more sources of\n",
        "# background clips.\n",
        "import os\n",
        "from microwakeword.audio.augmentation import Augmentation\n",
        "from microwakeword.audio.clips import Clips\n",
        "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
        "\n",
        "clips = Clips(input_directory='generated_samples',\n",
        "              file_pattern='*.wav',\n",
        "              max_clip_duration_s=None,\n",
        "              remove_silence=False,\n",
        "              random_split_seed=10,\n",
        "              split_count=0.1,\n",
        "              )\n",
        "\n",
        "# Updated augmentation settings for Colab with downloaded datasets\n",
        "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
        "                         augmentation_probabilities = {\n",
        "                                \"SevenBandParametricEQ\": 0.1,\n",
        "                                \"TanhDistortion\": 0.1,\n",
        "                                \"PitchShift\": 0.1,\n",
        "                                \"BandStopFilter\": 0.1,\n",
        "                                \"AddColorNoise\": 0.1,\n",
        "                                \"AddBackgroundNoise\": 0.75,\n",
        "                                \"Gain\": 1.0,\n",
        "                                \"RIR\": 0.5,\n",
        "                            },\n",
        "                         impulse_paths = [],  # Empty - no MIT RIR files downloaded\n",
        "                         # Updated to use ESC-50 instead of audioset (and FMA if available)\n",
        "                         background_paths = ['esc50_16k', 'fma_16k'] if os.path.exists('fma_16k') else ['esc50_16k'],\n",
        "                         background_min_snr_db = -5,\n",
        "                         background_max_snr_db = 10,\n",
        "                         min_jitter_s = 0.195,\n",
        "                         max_jitter_s = 0.205,\n",
        "                         )\n",
        "\n",
        "print(\"‚úÖ Augmentation setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5UsJfKKD1k9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if directory exists\n",
        "if os.path.exists('generated_samples'):\n",
        "    print(\"‚úÖ generated_samples directory exists\")\n",
        "\n",
        "    # Check for WAV files recursively\n",
        "    wav_files = list(Path('generated_samples').rglob('*.wav'))\n",
        "    print(f\"üìä Found {len(wav_files)} WAV files\")\n",
        "\n",
        "    if wav_files:\n",
        "        print(\"\\nüìÇ Sample file locations:\")\n",
        "        for f in wav_files[:5]:  # Show first 5\n",
        "            print(f\"  - {f}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No WAV files found in generated_samples\")\n",
        "else:\n",
        "    print(\"‚ùå generated_samples directory does NOT exist\")\n",
        "    print(\"\\nüí° You need to run the sample generation cell first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BHcY1mEGbK"
      },
      "outputs": [],
      "source": [
        "# Augment samples and save the training, validation, and testing sets.\n",
        "# Validating and testing samples generated the same way can make the model\n",
        "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
        "# samples generated with a different TTS engine to potentially get more accurate\n",
        "# benchmarks.\n",
        "\n",
        "import os\n",
        "from mmap_ninja.ragged import RaggedMmap\n",
        "\n",
        "output_dir = 'generated_augmented_features'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "splits = [\"training\", \"validation\", \"testing\"]\n",
        "for split in splits:\n",
        "  out_dir = os.path.join(output_dir, split)\n",
        "  if not os.path.exists(out_dir):\n",
        "      os.mkdir(out_dir)\n",
        "\n",
        "\n",
        "  split_name = \"train\"\n",
        "  repetition = 2\n",
        "\n",
        "  spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "  if split == \"validation\":\n",
        "    split_name = \"validation\"\n",
        "    repetition = 1\n",
        "  elif split == \"testing\":\n",
        "    split_name = \"test\"\n",
        "    repetition = 1\n",
        "    spectrograms = SpectrogramGeneration(clips=clips,\n",
        "                                     augmenter=augmenter,\n",
        "                                     slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
        "                                     step_ms=10,\n",
        "                                     )\n",
        "\n",
        "  RaggedMmap.from_generator(\n",
        "      out_dir=os.path.join(out_dir, 'wakeword_mmap'),\n",
        "      sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
        "      batch_size=100,\n",
        "      verbose=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pGuJDPyp3ax"
      },
      "outputs": [],
      "source": [
        "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
        "# particular) for various negative datasets. This can be slow!\n",
        "\n",
        "output_dir = './negative_datasets'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
        "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
        "    for fname in filenames:\n",
        "        link = link_root + fname\n",
        "\n",
        "        zip_path = f\"negative_datasets/{fname}\"\n",
        "        !wget -O {zip_path} {link}\n",
        "        !unzip -q {zip_path} -d {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii1A14GsGVQT"
      },
      "outputs": [],
      "source": [
        "# Save a yaml config that controls the training process\n",
        "# These hyperparamters can make a huge different in model quality.\n",
        "# Experiment with sampling and penalty weights and increasing the number of\n",
        "# training steps.\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "config = {}\n",
        "\n",
        "config[\"window_step_ms\"] = 10\n",
        "\n",
        "config[\"train_dir\"] = (\n",
        "    \"trained_models/wakeword\"\n",
        ")\n",
        "\n",
        "\n",
        "# Each feature_dir should have at least one of the following folders with this structure:\n",
        "#  training/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  testing_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#  validation_ambient/\n",
        "#    ragged_mmap_folders_ending_in_mmap\n",
        "#\n",
        "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
        "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
        "#  truth: Boolean whether this set has positive samples or negative samples\n",
        "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
        "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
        "#       - truncate_start: remove the start of the spectrogram\n",
        "#       - truncate_end: remove the end of the spectrogram\n",
        "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
        "\n",
        "config[\"features\"] = [\n",
        "    {\n",
        "        \"features_dir\": \"generated_augmented_features\",\n",
        "        \"sampling_weight\": 2.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": True,\n",
        "        \"truncation_strategy\": \"truncate_start\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/speech\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/dinner_party\",\n",
        "        \"sampling_weight\": 10.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    {\n",
        "        \"features_dir\": \"negative_datasets/no_speech\",\n",
        "        \"sampling_weight\": 5.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"random\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "    { # Only used for validation and testing\n",
        "        \"features_dir\": \"negative_datasets/dinner_party_eval\",\n",
        "        \"sampling_weight\": 0.0,\n",
        "        \"penalty_weight\": 1.0,\n",
        "        \"truth\": False,\n",
        "        \"truncation_strategy\": \"split\",\n",
        "        \"type\": \"mmap\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
        "config[\"training_steps\"] = [10000]\n",
        "\n",
        "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
        "config[\"positive_class_weight\"] = [1]\n",
        "config[\"negative_class_weight\"] = [20]\n",
        "\n",
        "config[\"learning_rates\"] = [\n",
        "    0.001,\n",
        "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
        "config[\"batch_size\"] = 128\n",
        "\n",
        "config[\"time_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_max_size\"] = [\n",
        "    0\n",
        "]  # SpecAugment - list that corresponds to training steps\n",
        "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
        "\n",
        "config[\"eval_step_interval\"] = (\n",
        "    500  # Test the validation sets after every this many steps\n",
        ")\n",
        "config[\"clip_duration_ms\"] = (\n",
        "    1500  # Maximum length of wake word that the streaming model will accept\n",
        ")\n",
        "\n",
        "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
        "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
        "# Available metrics:\n",
        "#   - \"loss\" - cross entropy error on validation set\n",
        "#   - \"accuracy\" - accuracy of validation set\n",
        "#   - \"recall\" - recall of validation set\n",
        "#   - \"precision\" - precision of validation set\n",
        "#   - \"false_positive_rate\" - false positive rate of validation set\n",
        "#   - \"false_negative_rate\" - false negative rate of validation set\n",
        "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
        "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
        "config[\"target_minimization\"] = 0.9\n",
        "config[\"minimization_metric\"] = None  # Set to None to disable\n",
        "\n",
        "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
        "\n",
        "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
        "    documents = yaml.dump(config, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoEXJBaiC9mf"
      },
      "outputs": [],
      "source": [
        "# Trains a model. When finished, it will quantize and convert the model to a\n",
        "# streaming version suitable for on-device detection.\n",
        "# It will resume if stopped, but it will start over at the configured training\n",
        "# steps in the yaml file.\n",
        "# Change --train 0 to only convert and test the best-weighted model.\n",
        "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
        "# stuck for several minutes! Additionally, it is very slow compared to training\n",
        "# on a local GPU.\n",
        "\n",
        "!python -m microwakeword.model_train_eval \\\n",
        "--training_config='training_parameters.yaml' \\\n",
        "--train 1 \\\n",
        "--restore_checkpoint 1 \\\n",
        "--test_tf_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming 0 \\\n",
        "--test_tflite_nonstreaming_quantized 0 \\\n",
        "--test_tflite_streaming 0 \\\n",
        "--test_tflite_streaming_quantized 1 \\\n",
        "--use_weights \"best_weights\" \\\n",
        "mixednet \\\n",
        "--pointwise_filters \"64,64,64,64\" \\\n",
        "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
        "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
        "--residual_connection \"0,0,0,0\" \\\n",
        "--first_conv_filters 32 \\\n",
        "--first_conv_kernel_size 5 \\\n",
        "--stride 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex_UIWvwtjAN"
      },
      "outputs": [],
      "source": [
        "# Downloads the tflite model file. To use on the device, you need to write a\n",
        "# Model JSON file. See https://esphome.io/components/micro_wake_word for the\n",
        "# documentation and\n",
        "# https://github.com/esphome/micro-wake-word-models/tree/main/models/v2 for\n",
        "# examples. Adjust the probability threshold based on the test results obtained\n",
        "# after training is finished. You may also need to increase the Tensor arena\n",
        "# model size if the model fails to load.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(f\"trained_models/wakeword/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}